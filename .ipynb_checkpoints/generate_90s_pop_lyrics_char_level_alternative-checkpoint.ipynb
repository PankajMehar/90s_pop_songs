{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "activations = 128\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "max_char_n = 30\n",
    "n_training = 100000\n",
    "training_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate logging variables\n",
    "variant = '{}ex-{}a-{}b-{}c'.format(n_training, activations, batch_size, max_char_n)\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)\n",
    "diagram_dir = 'diagram/{}-{}'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a csv into an array of song lyrics (None, 1)\n",
    "def csvToSongLyricsArray(csv):\n",
    "    # filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "    mask = (csv['year'] > 1989) & (csv['year'] < 2000) & (csv['genre'] == 'Pop') & (csv['lyrics'] != '[Instrumental]')\n",
    "    filtered = csv[mask]\n",
    "    \n",
    "    # remove null values\n",
    "    nonNull = filtered.dropna()\n",
    "    \n",
    "    # trim all the extra data. We only want the lyrics\n",
    "    lyrics = nonNull['lyrics']\n",
    "    \n",
    "    # reindex the lyrics to make it easier to work with\n",
    "    reindexed = lyrics.reset_index(drop=True)\n",
    "    \n",
    "    # lowercase the lyrics\n",
    "    lowercased = reindexed[:].str.lower()\n",
    "    \n",
    "    # get the number of song lyrics\n",
    "    n_songs = lowercased.shape[0]\n",
    "    \n",
    "    return lowercased, n_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocessPrediction(ix_to_char, prediction):\n",
    "    index = np.argmax(prediction)\n",
    "    char = ix_to_char[index]\n",
    "    \n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any song where lyrics contain a character outside the chars set\n",
    "def filterLyrics(chars, lyrics):\n",
    "    filtered_lyrics = []\n",
    "    \n",
    "    # for each song\n",
    "    for lyric in lyrics:\n",
    "        check = 0\n",
    "        \n",
    "        # split the lyric into an array of characters\n",
    "        lyric_chars = list(lyric)\n",
    "        \n",
    "        # for each character, check if it's not in the chars set\n",
    "        for char in lyric_chars:\n",
    "            if char not in chars:\n",
    "                check = 1\n",
    "\n",
    "        # if all character are in the chars set\n",
    "        # add it to our filter lyrics list\n",
    "        if check == 0:\n",
    "            filtered_lyrics.append(lyric_chars)\n",
    "            \n",
    "    # get the number of lyrics\n",
    "    n_filtered_lyrics = len(filtered_lyrics)    \n",
    "    \n",
    "    return filtered_lyrics, n_filtered_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the previous into a list of song lyrics lines\n",
    "def flatten_lyrics(lyrics):\n",
    "    flattened_lyrics = [line for song in lyrics for line in song]\n",
    "    n_chars = len(flattened_lyrics)\n",
    "    \n",
    "    return flattened_lyrics, n_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCharacterConverters(chars):\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "    \n",
    "    return char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(lyrics, n_chars, n_examples, max_char_n):\n",
    "    dataset = []\n",
    "    max_index = n_chars - max_char_n\n",
    "    start_indices = np.random.randint(0, max_index, size=n_examples)\n",
    "\n",
    "    for start_index in start_indices:\n",
    "        end_index = start_index + max_char_n\n",
    "        example = lyrics[start_index:end_index]\n",
    "        start_index = end_index\n",
    "        dataset.append(example)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXYDatasets(char_to_ix, dataset, n_examples, n_chars_set, max_char_n):\n",
    "    # create training input\n",
    "    X_training = np.zeros((n_examples, max_char_n-1, n_chars_set), dtype='float32')\n",
    "    X_training.shape\n",
    "    \n",
    "    # create training input\n",
    "    Y_training = np.zeros((n_examples, n_chars_set), dtype='float32')\n",
    "    Y_training.shape\n",
    "    \n",
    "    # fill input training set with word sequences, where words are one-hot encoded\n",
    "    for li, line in enumerate(dataset):\n",
    "        for ci, char in enumerate(line[:-1]):\n",
    "            index = char_to_ix[char]\n",
    "            X_training[li][ci][index] = 1\n",
    "            \n",
    "    # create training output\n",
    "    for li, line in enumerate(dataset):\n",
    "        char = line[-1]\n",
    "        index = char_to_ix[char]\n",
    "        Y_training[li][index] = 1\n",
    "        \n",
    "    return X_training, Y_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessExample(char_to_ix, example, n_chars_set):\n",
    "    chars = list(example)\n",
    "    n_sample_chars = len(chars)\n",
    "\n",
    "    preprocessed_example = np.zeros((1, n_sample_chars, n_chars_set), dtype='float32')\n",
    "\n",
    "    for ci, char in enumerate(chars):\n",
    "        index = char_to_ix[char]\n",
    "        preprocessed_example[0][ci][index] = 1\n",
    "\n",
    "    return preprocessed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get formatted_lyrics and number of songs\n",
    "lyrics, n_lyrics = csvToSongLyricsArray(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lyrics: 964\n",
      "Lyric Example: come they told me, pa rum pum pum pum\n",
      "a new born king to see, pa rum pum pum pum\n",
      "our finest gifts we bring, pa rum pum pum pum\n",
      "to lay before the king, pa rum pum pum pum,\n",
      "rum pum pum pum, rum pum pum pum,\n",
      "so to honor him, pa rum pum pum pum,\n",
      "when we come.\n",
      "little baby, pa rum pum pum pum\n",
      "i am a poor boy too, pa rum pum pum pum\n",
      "i have no gift to bring, pa rum pum pum pum\n",
      "that's fit to give the king, pa rum pum pum pum,\n",
      "rum pum pum pum, rum pum pum pum,\n",
      "shall i play for you, pa rum pum pum pum,\n",
      "on my drum?\n",
      "mary nodded, pa rum pum pum pum\n",
      "the ox and lamb kept time, pa rum pum pum pum\n",
      "i played my drum for him, pa rum pum pum pum,\n",
      "rum pum pum pum, rum pum pum pum,\n",
      "then he smiled at me, pa rum pum pum pum\n",
      "me and my drum\n",
      "me and my drum\n",
      "me and my drum\n",
      "me and my drum\n",
      "come they told me, pa rum pum pum pum\n",
      "me and my drum\n"
     ]
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "print(\"Number of lyrics: {}\".format(n_lyrics))\n",
    "print(\"Lyric Example: {}\".format(lyrics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-english lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '\\n', '!', '\"', '$', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in chars: 60\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars_set = len(chars)\n",
    "print(\"Number of characters in chars: {}\".format(n_chars_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out any song where lyrics contain a character outside the english set\n",
    "filtered_lyrics, n_filtered_lyrics = filterLyrics(chars, lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english songs: 782\n",
      "A english song lyric: ['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'a', ' ', 'n', 'e', 'w', ' ', 'b', 'o', 'r', 'n', ' ', 'k', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'e', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'o', 'u', 'r', ' ', 'f', 'i', 'n', 'e', 's', 't', ' ', 'g', 'i', 'f', 't', 's', ' ', 'w', 'e', ' ', 'b', 'r', 'i', 'n', 'g', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'o', ' ', 'l', 'a', 'y', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'k', 'i', 'n', 'g', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 's', 'o', ' ', 't', 'o', ' ', 'h', 'o', 'n', 'o', 'r', ' ', 'h', 'i', 'm', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 'w', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'c', 'o', 'm', 'e', '.', '\\n', 'l', 'i', 't', 't', 'l', 'e', ' ', 'b', 'a', 'b', 'y', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'a', 'm', ' ', 'a', ' ', 'p', 'o', 'o', 'r', ' ', 'b', 'o', 'y', ' ', 't', 'o', 'o', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'h', 'a', 'v', 'e', ' ', 'n', 'o', ' ', 'g', 'i', 'f', 't', ' ', 't', 'o', ' ', 'b', 'r', 'i', 'n', 'g', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'h', 'a', 't', \"'\", 's', ' ', 'f', 'i', 't', ' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 't', 'h', 'e', ' ', 'k', 'i', 'n', 'g', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 's', 'h', 'a', 'l', 'l', ' ', 'i', ' ', 'p', 'l', 'a', 'y', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 'o', 'n', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '?', '\\n', 'm', 'a', 'r', 'y', ' ', 'n', 'o', 'd', 'd', 'e', 'd', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'h', 'e', ' ', 'o', 'x', ' ', 'a', 'n', 'd', ' ', 'l', 'a', 'm', 'b', ' ', 'k', 'e', 'p', 't', ' ', 't', 'i', 'm', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'p', 'l', 'a', 'y', 'e', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 'm', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ',', '\\n', 't', 'h', 'e', 'n', ' ', 'h', 'e', ' ', 's', 'm', 'i', 'l', 'e', 'd', ' ', 'a', 't', ' ', 'm', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(\"Number of english songs: {}\".format(n_filtered_lyrics))\n",
    "print(\"A english song lyric: {}\".format(filtered_lyrics[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten english song lyrics\n",
    "flattened_lyrics, n_chars = flatten_lyrics(filtered_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of song lyrics characters: 846434\n",
      "Section of song lyrics: ['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'a', ' ', 'n', 'e', 'w', ' ', 'b', 'o', 'r', 'n', ' ', 'k', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'e', 'e', ',', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'o', 'u', 'r', ' ', 'f', 'i', 'n', 'e', 's', 't', ' ', 'g', 'i', 'f', 't', 's', ' ', 'w', 'e']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of song lyrics characters: {}\".format(n_chars))\n",
    "print(\"Section of song lyrics: {}\".format(flattened_lyrics[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in dataset: 2000\n",
      "Example: [' ', '(', 'i', 'n', 't', 'e', 'n', 't', 'i', 'o', 'n', ')', '\\n', 'l', 'i', 's', 't', 'e', 'n', ' ', 'a', 't', ' ', 'y', 'o', 'u', 'r', ' ', 'p', 'e']\n"
     ]
    }
   ],
   "source": [
    "# generate n_training example of max_char_n length\n",
    "dataset = generateDataset(flattened_lyrics, n_chars, n_training, max_char_n)\n",
    "print(\"Number of examples in dataset: {}\".format(len(dataset)))\n",
    "print(\"Example: {}\".format(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ix_to_char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: ':', 17: ';', 18: '<', 19: '=', 20: '>', 21: '?', 22: '@', 23: '[', 24: '\\\\', 25: ']', 26: '^', 27: '_', 28: '`', 29: 'a', 30: 'b', 31: 'c', 32: 'd', 33: 'e', 34: 'f', 35: 'g', 36: 'h', 37: 'i', 38: 'j', 39: 'k', 40: 'l', 41: 'm', 42: 'n', 43: 'o', 44: 'p', 45: 'q', 46: 'r', 47: 's', 48: 't', 49: 'u', 50: 'v', 51: 'w', 52: 'x', 53: 'x', 54: 'y', 55: 'z', 56: '{', 57: '|', 58: '}', 59: '~'}\n",
      "char_to_ix: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, ':': 16, ';': 17, '<': 18, '=': 19, '>': 20, '?': 21, '@': 22, '[': 23, '\\\\': 24, ']': 25, '^': 26, '_': 27, '`': 28, 'a': 29, 'b': 30, 'c': 31, 'd': 32, 'e': 33, 'f': 34, 'g': 35, 'h': 36, 'i': 37, 'j': 38, 'k': 39, 'l': 40, 'm': 41, 'n': 42, 'o': 43, 'p': 44, 'q': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'x': 53, 'y': 54, 'z': 55, '{': 56, '|': 57, '}': 58, '~': 59}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix, ix_to_char = generateCharacterConverters(chars)\n",
    "print(\"ix_to_char: {}\".format(ix_to_char))\n",
    "print(\"char_to_ix: {}\".format(char_to_ix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and Y Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training input and output\n",
    "X_training, Y_training = generateXYDatasets(char_to_ix, dataset, n_training, n_chars_set, max_char_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape: (29, 60)\n",
      "X_training example one-hot: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "X_training example: er found\n",
      "but it's waiting her\n"
     ]
    }
   ],
   "source": [
    "x_example = X_training[2] \n",
    "\n",
    "x_training_string = []\n",
    "for woh in x_example:\n",
    "    char = deprocessPrediction(ix_to_char, woh)\n",
    "    x_training_string.append(char)\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(\"X_training shape: {}\".format(x_example.shape))\n",
    "print(\"X_training example one-hot: {}\".format(x_example))\n",
    "print(\"X_training example: {}\".format(x_training_string_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_training shape: (60,)\n",
      "Y_training example one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Y_training example: e\n"
     ]
    }
   ],
   "source": [
    "y_example = Y_training[2]\n",
    "\n",
    "char = deprocessPrediction(ix_to_char, y_example)\n",
    "print(\"Y_training shape: {}\".format(y_example.shape))\n",
    "print(\"Y_training example one-hot: {}\".format(y_example))\n",
    "print(\"Y_training example: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars_set))\n",
    "x = LSTM(activations)(model_input)\n",
    "x = Dense(n_chars_set, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/50\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 3.2841 - acc: 0.1657 - val_loss: 3.1024 - val_acc: 0.1700\n",
      "Epoch 2/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 2.9687 - acc: 0.2029 - val_loss: 3.0015 - val_acc: 0.1733\n",
      "Epoch 3/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 2.7791 - acc: 0.2400 - val_loss: 2.9028 - val_acc: 0.1983\n",
      "Epoch 4/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 2.5490 - acc: 0.2821 - val_loss: 2.7720 - val_acc: 0.2167\n",
      "Epoch 5/50\n",
      "1400/1400 [==============================] - 1s 1ms/step - loss: 2.4235 - acc: 0.3136 - val_loss: 2.6350 - val_acc: 0.2467\n",
      "Epoch 6/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 2.2728 - acc: 0.3229 - val_loss: 2.5767 - val_acc: 0.2883\n",
      "Epoch 7/50\n",
      "1400/1400 [==============================] - 1s 1ms/step - loss: 2.1628 - acc: 0.3557 - val_loss: 2.5287 - val_acc: 0.2950\n",
      "Epoch 8/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 2.0368 - acc: 0.3893 - val_loss: 2.5332 - val_acc: 0.3017\n",
      "Epoch 9/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 1.9221 - acc: 0.4157 - val_loss: 2.6191 - val_acc: 0.2867\n",
      "Epoch 10/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.7853 - acc: 0.4529 - val_loss: 2.6363 - val_acc: 0.2900\n",
      "Epoch 11/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.6386 - acc: 0.4843 - val_loss: 2.6598 - val_acc: 0.3067\n",
      "Epoch 12/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.4988 - acc: 0.5271 - val_loss: 2.7467 - val_acc: 0.2817\n",
      "Epoch 13/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.3189 - acc: 0.5921 - val_loss: 2.8368 - val_acc: 0.2933\n",
      "Epoch 14/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 1.1759 - acc: 0.6279 - val_loss: 2.9352 - val_acc: 0.2867\n",
      "Epoch 15/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.9684 - acc: 0.6929 - val_loss: 3.0058 - val_acc: 0.2950\n",
      "Epoch 16/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.8031 - acc: 0.7700 - val_loss: 3.1755 - val_acc: 0.2783\n",
      "Epoch 17/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.6667 - acc: 0.8143 - val_loss: 3.3100 - val_acc: 0.2850\n",
      "Epoch 18/50\n",
      "1400/1400 [==============================] - 2s 2ms/step - loss: 0.5427 - acc: 0.8486 - val_loss: 3.3552 - val_acc: 0.2667\n",
      "Epoch 19/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.4428 - acc: 0.8971 - val_loss: 3.6111 - val_acc: 0.2683\n",
      "Epoch 20/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.3491 - acc: 0.9086 - val_loss: 3.7778 - val_acc: 0.2617\n",
      "Epoch 21/50\n",
      "1400/1400 [==============================] - 2s 1ms/step - loss: 0.3047 - acc: 0.9243 - val_loss: 3.8339 - val_acc: 0.2683\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f38c1cc7208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          Y_training, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'sweet dreams are made of thes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert example to a sequence of one-hot encoded chars\n",
    "preprocessed_example = preprocessExample(char_to_ix, example, n_chars_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(preprocessed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  \n"
     ]
    }
   ],
   "source": [
    "char = deprocessPrediction(ix_to_char, prediction[0])\n",
    "print(\"Prediction: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of thes the i'ghen you'd deyore sayo withan to cole cole come butt than than cale to tele you'd (ieyo bore "
     ]
    }
   ],
   "source": [
    "sys.stdout.write(example)\n",
    "\n",
    "for i in range(100):\n",
    "    prediction = model.predict(preprocessed_example, verbose=0)[0]\n",
    "    sampled_prediction = sample_predictions(prediction)\n",
    "    next_char = deprocessPrediction(ix_to_char, sampled_prediction[0])\n",
    "    preprocessed_example[0][:-1] = preprocessed_example[0][1:]\n",
    "    preprocessed_example[0][-1] = sampled_prediction\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 133.00 191.00\" width=\"133pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 129,-187 129,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139881749448800 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139881749448800</title>\n",
       "<polygon fill=\"none\" points=\"-7.10543e-15,-146.5 -7.10543e-15,-182.5 125,-182.5 125,-146.5 -7.10543e-15,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139881749449248 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139881749449248</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-73.5 13.5,-109.5 111.5,-109.5 111.5,-73.5 13.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139881749448800&#45;&gt;139881749449248 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139881749448800-&gt;139881749449248</title>\n",
       "<path d=\"M62.5,-146.313C62.5,-138.289 62.5,-128.547 62.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-119.529 62.5,-109.529 59.0001,-119.529 66.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139881749450480 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139881749450480</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-0.5 11.5,-36.5 113.5,-36.5 113.5,-0.5 11.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139881749449248&#45;&gt;139881749450480 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139881749449248-&gt;139881749450480</title>\n",
       "<path d=\"M62.5,-73.3129C62.5,-65.2895 62.5,-55.5475 62.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-46.5288 62.5,-36.5288 59.0001,-46.5289 66.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a model diagram and save it to disk\n",
    "plot_model(model, to_file=diagram_dir)\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
