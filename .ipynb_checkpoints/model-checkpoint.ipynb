{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coremltools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b594a378b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcoremltools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coremltools'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from coremltools import converters\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "activations = 128\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "training_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "output_dir = 'data/'\n",
    "charset_file = '{}charset.csv'.format(output_dir)\n",
    "dataset_file = '{}dataset.csv'.format(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocessPrediction(ix_to_char, prediction):\n",
    "    index = np.argmax(prediction)\n",
    "    char = ix_to_char[index]\n",
    "    \n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCharacterConverters(chars):\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "    \n",
    "    return char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n):\n",
    "    # create training input\n",
    "    x_dataset = np.zeros((n_examples, max_char_n-1, n_charset), dtype='float32')\n",
    "    \n",
    "    # create training input\n",
    "    y_dataset = np.zeros((n_examples, n_charset), dtype='float32')\n",
    "    \n",
    "    # fill input training set with word sequences, where words are one-hot encoded\n",
    "    for li, line in enumerate(dataset):\n",
    "        for ci, char in enumerate(line[:-1]):\n",
    "            index = char_to_ix[char]\n",
    "            x_dataset[li][ci][index] = 1\n",
    "            \n",
    "    # create training output\n",
    "    for li, line in enumerate(dataset):\n",
    "        char = line[-1]\n",
    "        index = char_to_ix[char]\n",
    "        y_dataset[li][index] = 1\n",
    "        \n",
    "    return x_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessExample(char_to_ix, example, n_chars_set):\n",
    "    chars = list(example)\n",
    "    n_sample_chars = len(chars)\n",
    "\n",
    "    preprocessed_example = np.zeros((1, n_sample_chars, n_chars_set), dtype='float32')\n",
    "\n",
    "    for ci, char in enumerate(chars):\n",
    "        index = char_to_ix[char]\n",
    "        preprocessed_example[0][ci][index] = 1\n",
    "\n",
    "    return preprocessed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(charset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    charset = []\n",
    "    for row in reader:\n",
    "        charset.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    dataset = []\n",
    "    for row in reader:\n",
    "        dataset.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionarys\n",
    "char_to_ix, ix_to_char = generateCharacterConverters(charset)\n",
    "n_charset = len(charset)\n",
    "\n",
    "print(\"Number of characters: {}\".format(n_charset))\n",
    "print(\"ix_to_char: {}\".format(ix_to_char))\n",
    "print(\"char_to_ix: {}\".format(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training input and output\n",
    "n_examples = len(dataset)\n",
    "max_char_n = len(dataset[0])\n",
    "x_dataset, y_dataset = generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n)\n",
    "print(\"Number of examples: {}\".format(n_examples))\n",
    "print(\"max characters: {}\".format(max_char_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_example = x_dataset[2] \n",
    "\n",
    "x_example_string = []\n",
    "for woh in x_example:\n",
    "    char = deprocessPrediction(ix_to_char, woh)\n",
    "    x_example_string.append(char)\n",
    "x_example_string_formatted = \"\".join(x_example_string)\n",
    "\n",
    "print(\"x_example shape: {}\".format(x_example.shape))\n",
    "print(\"x_example one-hot: {}\".format(x_example))\n",
    "print(\"x_example: {}\".format(x_example_string_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_example = y_dataset[2]\n",
    "\n",
    "char = deprocessPrediction(ix_to_char, y_example)\n",
    "\n",
    "print(\"y_example shape: {}\".format(y_example.shape))\n",
    "print(\"y_example one-hot: {}\".format(y_example))\n",
    "print(\"y_example: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_charset))\n",
    "x = LSTM(activations)(model_input)\n",
    "x = Dense(n_charset, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate logging variables\n",
    "variant = '{}ex-{}a-{}b-{}c'.format(n_examples, activations, batch_size, max_char_n)\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "diagram_dir = 'diagram/{}-{}'.format(variant, timestamp)\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)\n",
    "mlmodel_dir = 'model/{}-{}.mlmodel'.format(variant, timestamp)\n",
    "model_dir = 'model/{}-{}.hdf5'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a model diagram and save it to disk\n",
    "plot_model(model, to_file=diagram_dir)\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_dataset, \n",
    "          y_dataset, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = converters.keras.conver(model)\n",
    "coreml_model.save(mlmodel_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
