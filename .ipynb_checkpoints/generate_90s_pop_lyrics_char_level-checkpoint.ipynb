{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "n_training = 30000\n",
    "training_ratio = 0.3\n",
    "\n",
    "# logging name\n",
    "variant = '0.7t-30000-100b'\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "mask = (raw_data['year'] > 1989) & (raw_data['year'] < 2000) & (raw_data['genre'] == 'Pop') & (raw_data['lyrics'] != '[Instrumental]')\n",
    "filtered_data = raw_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any that have null values\n",
    "cleaned_data = filtered_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all the extra data. We only want the lyrics\n",
    "raw_lyrics = cleaned_data['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the lyrics to make it easier to work with\n",
    "reindexed_lyrics = raw_lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the lyrics to make it easier to work with\n",
    "formatted_lyrics = reindexed_lyrics[:].str.lower()\n",
    "formatted_lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "n_formatted_lyrics = formatted_lyrics.shape[0]\n",
    "print(n_formatted_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each lyric on \\n\n",
    "# store song lyrics as a list of lines\n",
    "# store those in lyrics\n",
    "lyrics_lines = []\n",
    "\n",
    "for i in range(n_formatted_lyrics):\n",
    "    lyrics = formatted_lyrics[i].split('\\n')\n",
    "    lyrics_lines.append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the previous into a list of song lyrics lines\n",
    "flattened_lyrics_lines = [line for song in lyrics_lines for line in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35188\n",
      "come they told me, pa rum pum pum pum\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(flattened_lyrics_lines))\n",
    "print(flattened_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-english lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_lyrics_lines = []\n",
    "\n",
    "for line in flattened_lyrics_lines:\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    char_check = 0\n",
    "    for char in char_split:\n",
    "        if char not in char_set:\n",
    "            char_check = 1\n",
    "            \n",
    "    if char_check == 0:\n",
    "        english_lyrics_lines.append(char_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33460\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(english_lyrics_lines))\n",
    "print(english_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['t', 'o', ' ', 't', 'h', 'e', ' ', 'd', 'a', 'r', 'k', ' ', 'a', 'n', 'd', ' ', 'e', 'm', 'p', 't', 'y', ' ', 's', 'k', 'i', 'e', 's']\n"
     ]
    }
   ],
   "source": [
    "# grab a random amount of them for our examples\n",
    "examples = sample(english_lyrics_lines, n_training)\n",
    "\n",
    "print(len(examples))\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process lyrics into lists of word indices\n",
    "# also determine line with the greatest length\n",
    "max_char_n = 0\n",
    "\n",
    "for line in examples:\n",
    "    char_n = len(line)\n",
    "    if char_n > max_char_n:\n",
    "        max_char_n = char_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "print(max_char_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'y', 'v', 'r', 's', 'm', \"'\", 'e', 'c', 'b', 'f', 't', 'i', 'j', 'z', 'g', 'h', 'p', ' ', 'd', 'w', 'q', 'n', 'u', 'k', 'a', 'x', 'o']\n"
     ]
    }
   ],
   "source": [
    "# flatten chars\n",
    "flat_chars = [item for sublist in examples for item in sublist]\n",
    "\n",
    "# dedup list\n",
    "chars = list(set(flat_chars))\n",
    "\n",
    "# append our terminator\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: \"'\", 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n",
      "{' ': 0, \"'\": 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 69, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training input\n",
    "X_training = np.zeros((n_training, max_char_n, n_chars), dtype='float32')\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill input training set with word sequences, where words are one-hot encoded\n",
    "for li, line in enumerate(examples[:n_training]):\n",
    "    indices = []\n",
    "    for ci, char in enumerate(line):\n",
    "        index = char_to_ix[char]\n",
    "        X_training[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training output\n",
    "Y_training = X_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the dark and empty skies                                          \n"
     ]
    }
   ],
   "source": [
    "x_training_string = []\n",
    "for woh in X_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    x_training_string.append(ix_to_char[max_idx])\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to the dark and empty skies                                          \n"
     ]
    }
   ],
   "source": [
    "y_training_string = []\n",
    "for woh in Y_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    y_training_string.append(ix_to_char[max_idx])\n",
    "y_training_string_formatted = \"\".join(y_training_string)\n",
    "print(y_training_string_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars))\n",
    "x = LSTM(128, return_sequences=True)(model_input)\n",
    "x = Dense(n_chars, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "700/700 [==============================] - 3s 4ms/step - loss: 1.1352 - acc: 0.4498 - val_loss: 0.6846 - val_acc: 0.7299\n",
      "Epoch 2/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.2828 - acc: 0.9631 - val_loss: 0.0732 - val_acc: 0.9987\n",
      "Epoch 3/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0415 - acc: 0.9992 - val_loss: 0.0234 - val_acc: 0.9999\n",
      "Epoch 4/50\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0148 - acc: 0.9998 - val_loss: 0.0091 - val_acc: 1.0000\n",
      "Epoch 5/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0060 - acc: 0.9998 - val_loss: 0.0037 - val_acc: 0.9999\n",
      "Epoch 6/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0024 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 7/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 9.8622e-04 - acc: 0.9999 - val_loss: 6.1486e-04 - val_acc: 1.0000\n",
      "Epoch 8/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 4.1023e-04 - acc: 1.0000 - val_loss: 2.6116e-04 - val_acc: 1.0000\n",
      "Epoch 9/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 1.7500e-04 - acc: 1.0000 - val_loss: 1.1356e-04 - val_acc: 1.0000\n",
      "Epoch 10/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 7.6475e-05 - acc: 1.0000 - val_loss: 4.9518e-05 - val_acc: 1.0000\n",
      "Epoch 11/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 3.3865e-05 - acc: 1.0000 - val_loss: 2.2678e-05 - val_acc: 1.0000\n",
      "Epoch 12/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 1.5692e-05 - acc: 1.0000 - val_loss: 1.0799e-05 - val_acc: 1.0000\n",
      "Epoch 13/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 7.5390e-06 - acc: 1.0000 - val_loss: 5.3469e-06 - val_acc: 1.0000\n",
      "Epoch 14/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 3.7907e-06 - acc: 1.0000 - val_loss: 2.8106e-06 - val_acc: 1.0000\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc6c106518>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          Y_training, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = 'sweet dreams are made of these'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert new_sample to a sequence of one-hot encoded chars\n",
    "line_split = text_to_word_sequence(new_sample, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "char_split = list(\" \".join(line_split))\n",
    "n_sample_chars = len(char_split)\n",
    "\n",
    "sample = np.zeros((1, n_sample_chars, n_chars), dtype='float32')\n",
    "\n",
    "for ci, char in enumerate(char_split):\n",
    "    index = char_to_ix[char]\n",
    "    sample[0][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in prediction[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prediction = \"\".join(string_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    x = np.zeros((1, prediction.shape[1] + 1, n_chars), dtype='float32')\n",
    "    x[0][:prediction.shape[1]] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these   o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o  o   "
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    next_char = ix_to_char[max_p]\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
