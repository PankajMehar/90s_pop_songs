{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "mask = (raw_data['year'] > 1989) & (raw_data['year'] < 2000) & (raw_data['genre'] == 'Pop') & (raw_data['lyrics'] != '[Instrumental]')\n",
    "filtered_data = raw_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any that have null values\n",
    "cleaned_data = filtered_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all the extra data. We only want the lyrics\n",
    "raw_lyrics = cleaned_data['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the lyrics to make it easier to work with\n",
    "reindexed_lyrics = raw_lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the lyrics to make it easier to work with\n",
    "formatted_lyrics = reindexed_lyrics[:].str.lower()\n",
    "formatted_lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "n_formatted_lyrics = formatted_lyrics.shape[0]\n",
    "print(n_formatted_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each lyric on \\n\n",
    "# store song lyrics as a list of lines\n",
    "# store those in lyrics\n",
    "lyrics_lines = []\n",
    "\n",
    "for i in range(n_formatted_lyrics):\n",
    "    lyrics = formatted_lyrics[i].split('\\n')\n",
    "    lyrics_lines.append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the previous into a list of song lyrics lines\n",
    "flattened_lyrics_lines = [line for song in lyrics_lines for line in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35188\n",
      "come they told me, pa rum pum pum pum\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(flattened_lyrics_lines))\n",
    "print(flattened_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-english lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "english_lyrics_lines = []\n",
    "\n",
    "for line in flattened_lyrics_lines:\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    char_check = 0\n",
    "    for char in char_split:\n",
    "        if char not in char_set:\n",
    "            char_check = 1\n",
    "            \n",
    "    if char_check == 0:\n",
    "        english_lyrics_lines.append(char_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33659\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(english_lyrics_lines))\n",
    "print(english_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['i', 'n', ' ', 's', 'p', 'i', 't', 'e', ' ', 'o', 'f', ' ', 'i', 't', ' ', 'a', 'l', 'l']\n"
     ]
    }
   ],
   "source": [
    "# grab a random amount of them for our examples\n",
    "from random import sample\n",
    "\n",
    "n_training = 7000\n",
    "n_validation = 3000\n",
    "\n",
    "examples = sample(english_lyrics_lines, n_training + n_validation)\n",
    "\n",
    "print(len(examples))\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process lyrics into lists of word indices\n",
    "# also determine line with the greatest length\n",
    "max_char_n = 0\n",
    "\n",
    "for line in examples:\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    char_n = len(line)\n",
    "    if char_n > max_char_n:\n",
    "        max_char_n = char_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "print(max_char_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['g', '9', '1', '6', 's', 'n', 'p', 'q', 'w', '2', 'i', 'o', 'r', 'x', 'c', 'l', '4', '5', 'j', 'u', 'h', '3', 'k', 'z', 'f', 'y', 'v', '8', 'b', ' ', 'a', \"'\", '0', 'd', 't', 'e', 'm', '7']\n"
     ]
    }
   ],
   "source": [
    "# flatten chars\n",
    "flat_chars = [item for sublist in examples for item in sublist]\n",
    "\n",
    "# dedup list\n",
    "chars = list(set(flat_chars))\n",
    "\n",
    "# append our terminator\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: \"'\", 2: '0', 3: '1', 4: '2', 5: '3', 6: '4', 7: '5', 8: '6', 9: '7', 10: '8', 11: '9', 12: 'a', 13: 'b', 14: 'c', 15: 'd', 16: 'e', 17: 'f', 18: 'g', 19: 'h', 20: 'i', 21: 'j', 22: 'k', 23: 'l', 24: 'm', 25: 'n', 26: 'o', 27: 'p', 28: 'q', 29: 'r', 30: 's', 31: 't', 32: 'u', 33: 'v', 34: 'w', 35: 'x', 36: 'y', 37: 'z'}\n",
      "{' ': 0, \"'\": 1, '0': 2, '1': 3, '2': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11, 'a': 12, 'b': 13, 'c': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'j': 21, 'k': 22, 'l': 23, 'm': 24, 'n': 25, 'o': 26, 'p': 27, 'q': 28, 'r': 29, 's': 30, 't': 31, 'u': 32, 'v': 33, 'w': 34, 'x': 35, 'y': 36, 'z': 37}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 108, 38)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create training input\n",
    "X_training = np.zeros((n_training, max_char_n, n_chars), dtype='float32')\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill input training set with word sequences, where words are one-hot encoded\n",
    "for li, line in enumerate(examples[:n_training]):\n",
    "    indices = []\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(line):\n",
    "        index = char_to_ix[char]\n",
    "        X_training[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training output\n",
    "# Y_training = np.resize(X_training, (n_training, max_char_n + 1, n_chars))\n",
    "# Y_training.shape\n",
    "Y_training = X_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs need to end with the end of sequence charecter\n",
    "# for li, line in enumerate(X_training):\n",
    "#     spaceCounter = 0\n",
    "#     for ci, char in enumerate(line):\n",
    "#         if np.all(Y_training[li][ci] == 0):\n",
    "#             spaceCounter += 1\n",
    "#         if spaceCounter > 1:\n",
    "#             Y_training[li][ci-1][-1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 108, 38)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation input\n",
    "X_validation = np.zeros((n_validation, max_char_n, n_chars), dtype='float32')\n",
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li, line in enumerate(examples[n_training:n_validation]):\n",
    "    indices = []\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(line):\n",
    "        index = char_to_ix[char]\n",
    "        X_validation[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation output\n",
    "# Y_validation = np.resize(X_validation, (n_validation, max_char_n + 1, n_chars))\n",
    "# Y_validation.shape\n",
    "Y_validation = X_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs need to end with the end of sequence charecter\n",
    "# for li, line in enumerate(X_validation):\n",
    "#     spaceCounter = 0\n",
    "#     for ci, char in enumerate(line):\n",
    "#         if np.all(Y_validation[li][ci] == 0):\n",
    "#             spaceCounter += 1\n",
    "#         if spaceCounter > 1:\n",
    "#             Y_validation[li][ci-1][-1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in spite of it all                                                                                          \n"
     ]
    }
   ],
   "source": [
    "x_training_string = []\n",
    "for woh in X_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    x_training_string.append(ix_to_char[max_idx])\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in spite of it all                                                                                          \n"
     ]
    }
   ],
   "source": [
    "y_training_string = []\n",
    "for woh in Y_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    y_training_string.append(ix_to_char[max_idx])\n",
    "y_training_string_formatted = \"\".join(y_training_string)\n",
    "print(y_training_string_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars))\n",
    "x = LSTM(n_chars, return_sequences=True)(model_input)\n",
    "x = Dense(n_chars, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}'.format(timestamp)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      "7000/7000 [==============================] - 30s 4ms/step - loss: 0.1405 - acc: 0.9359 - val_loss: 0.0000e+00 - val_acc: 0.9907\n",
      "Epoch 2/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 2.3095e-04 - acc: 0.8301 - val_loss: 0.0000e+00 - val_acc: 1.0000\n",
      "Epoch 3/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 9.5317e-05 - acc: 0.4725 - val_loss: 0.0000e+00 - val_acc: 0.0185\n",
      "Epoch 4/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 4.8395e-05 - acc: 0.3460 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 2.6368e-05 - acc: 0.2669 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 1.6298e-05 - acc: 0.2660 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 7.8585e-06 - acc: 0.2771 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 4.0553e-06 - acc: 0.4210 - val_loss: 0.0000e+00 - val_acc: 0.0556\n",
      "Epoch 9/50\n",
      "7000/7000 [==============================] - 29s 4ms/step - loss: 3.2991e-06 - acc: 0.3996 - val_loss: 0.0000e+00 - val_acc: 0.1944\n",
      "Epoch 10/50\n",
      "7000/7000 [==============================] - 29s 4ms/step - loss: 2.0118e-06 - acc: 0.2905 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 4.4859e-07 - acc: 0.3186 - val_loss: 0.0000e+00 - val_acc: 0.2685\n",
      "Epoch 12/50\n",
      "7000/7000 [==============================] - 28s 4ms/step - loss: 2.7349e-07 - acc: 0.2778 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c3f104908>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          Y_training, \n",
    "          batch_size=50, \n",
    "          epochs=50, \n",
    "          validation_data=(X_validation, Y_validation),\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = 'sweet dreams are made of '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert new_sample to a sequence of one-hot encoded chars\n",
    "line_split = text_to_word_sequence(new_sample, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "char_split = list(\" \".join(line_split))\n",
    "n_sample_chars = len(char_split)\n",
    "\n",
    "sample = np.zeros((1, n_sample_chars, n_chars), dtype='float32')\n",
    "\n",
    "for ci, char in enumerate(char_split):\n",
    "    index = char_to_ix[char]\n",
    "    sample[0][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in prediction[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prediction = \"\".join(string_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    x = np.zeros((1, prediction.shape[1] + 1, n_chars), dtype='float32')\n",
    "    x[0][:prediction.shape[1]][:] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made ofaze ai 4 5 667777     7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 \n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
