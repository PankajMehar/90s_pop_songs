{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "activations = 128\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "learning_rate = 0.01\n",
    "max_char_n = 40\n",
    "n_training = 10000\n",
    "training_ratio = 0.3\n",
    "\n",
    "# logging name\n",
    "variant = 'single_output'\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "mask = (raw_data['year'] > 1989) & (raw_data['year'] < 2000) & (raw_data['genre'] == 'Pop') & (raw_data['lyrics'] != '[Instrumental]')\n",
    "filtered_data = raw_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any that have null values\n",
    "cleaned_data = filtered_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all the extra data. We only want the lyrics\n",
    "raw_lyrics = cleaned_data['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the lyrics to make it easier to work with\n",
    "reindexed_lyrics = raw_lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the lyrics to make it easier to work with\n",
    "formatted_lyrics = reindexed_lyrics[:].str.lower()\n",
    "formatted_lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "n_formatted_lyrics = formatted_lyrics.shape[0]\n",
    "print(n_formatted_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-english lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_lyrics_lines = []\n",
    "\n",
    "for line in formatted_lyrics:\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    char_check = 0\n",
    "    for char in char_split:\n",
    "        if char not in char_set:\n",
    "            char_check = 1\n",
    "            \n",
    "    if char_check == 0:\n",
    "        english_lyrics_lines.append(char_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'a', ' ', 'n', 'e', 'w', ' ', 'b', 'o', 'r', 'n', ' ', 'k', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'o', 'u', 'r', ' ', 'f', 'i', 'n', 'e', 's', 't', ' ', 'g', 'i', 'f', 't', 's', ' ', 'w', 'e', ' ', 'b', 'r', 'i', 'n', 'g', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'o', ' ', 'l', 'a', 'y', ' ', 'b', 'e', 'f', 'o', 'r', 'e', ' ', 't', 'h', 'e', ' ', 'k', 'i', 'n', 'g', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 's', 'o', ' ', 't', 'o', ' ', 'h', 'o', 'n', 'o', 'r', ' ', 'h', 'i', 'm', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 'w', 'h', 'e', 'n', ' ', 'w', 'e', ' ', 'c', 'o', 'm', 'e', ' ', '\\n', 'l', 'i', 't', 't', 'l', 'e', ' ', 'b', 'a', 'b', 'y', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'a', 'm', ' ', 'a', ' ', 'p', 'o', 'o', 'r', ' ', 'b', 'o', 'y', ' ', 't', 'o', 'o', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'h', 'a', 'v', 'e', ' ', 'n', 'o', ' ', 'g', 'i', 'f', 't', ' ', 't', 'o', ' ', 'b', 'r', 'i', 'n', 'g', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'h', 'a', 't', \"'\", 's', ' ', 'f', 'i', 't', ' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 't', 'h', 'e', ' ', 'k', 'i', 'n', 'g', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 's', 'h', 'a', 'l', 'l', ' ', 'i', ' ', 'p', 'l', 'a', 'y', ' ', 'f', 'o', 'r', ' ', 'y', 'o', 'u', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 'o', 'n', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', ' ', '\\n', 'm', 'a', 'r', 'y', ' ', 'n', 'o', 'd', 'd', 'e', 'd', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 't', 'h', 'e', ' ', 'o', 'x', ' ', 'a', 'n', 'd', ' ', 'l', 'a', 'm', 'b', ' ', 'k', 'e', 'p', 't', ' ', 't', 'i', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'i', ' ', 'p', 'l', 'a', 'y', 'e', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 'm', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', '\\n', 't', 'h', 'e', 'n', ' ', 'h', 'e', ' ', 's', 'm', 'i', 'l', 'e', 'd', ' ', 'a', 't', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm', '\\n', 'c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'y', ' ', 'd', 'r', 'u', 'm']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(english_lyrics_lines))\n",
    "print(english_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the previous into a list of song lyrics lines\n",
    "flattened_lyrics = [line for song in english_lyrics_lines for line in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837528\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'a', ' ', 'n', 'e', 'w', ' ', 'b', 'o', 'r', 'n', ' ', 'k', 'i', 'n', 'g', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', '\\n', 'o', 'u', 'r', ' ', 'f', 'i', 'n', 'e', 's', 't', ' ', 'g', 'i', 'f', 't', 's', ' ', 'w', 'e', ' ', 'b']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(flattened_lyrics))\n",
    "print(flattened_lyrics[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a']\n"
     ]
    }
   ],
   "source": [
    "# generate n_training example of max_char_n length\n",
    "examples = []\n",
    "start_index = 0\n",
    "\n",
    "for i in range(n_training):\n",
    "    end_index = start_index + max_char_n\n",
    "    example = flattened_lyrics[start_index:end_index]\n",
    "    start_index = end_index\n",
    "    examples.append(example)\n",
    "\n",
    "print(len(examples))\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars = len(char_set)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: \"'\", 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'x', 28: 'y', 29: 'z'}\n",
      "{'\\n': 0, ' ': 1, \"'\": 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 27, 'y': 28, 'z': 29}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(char_set)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(char_set)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 19, 30)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training input\n",
    "X_training = np.zeros((n_training, max_char_n-1, n_chars), dtype='float32')\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill input training set with word sequences, where words are one-hot encoded\n",
    "for li, line in enumerate(examples):\n",
    "    for ci, char in enumerate(line[:-1]):\n",
    "        index = char_to_ix[char]\n",
    "        X_training[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training input\n",
    "Y_training = np.zeros((n_training, n_chars), dtype='float32')\n",
    "Y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training output\n",
    "for li, line in enumerate(examples):\n",
    "    char = line[-1]\n",
    "    index = char_to_ix[char]\n",
    "    Y_training[li][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape: (19, 30)\n",
      "X_training example one-hot: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "X_training example: come they told me p\n"
     ]
    }
   ],
   "source": [
    "x_training_string = []\n",
    "x_example = X_training[0] \n",
    "for woh in x_example:\n",
    "    max_idx = np.argmax(woh)\n",
    "    x_training_string.append(ix_to_char[max_idx])\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(\"X_training shape: {}\".format(x_example.shape))\n",
    "print(\"X_training example one-hot: {}\".format(x_example))\n",
    "print(\"X_training example: {}\".format(x_training_string_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_training shape: (30,)\n",
      "Y_training example one-hot: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Y_training example: a\n"
     ]
    }
   ],
   "source": [
    "y_example = Y_training[0]\n",
    "index = np.argmax(y_example)\n",
    "char = ix_to_char[index]\n",
    "print(\"Y_training shape: {}\".format(y_example.shape))\n",
    "print(\"Y_training example one-hot: {}\".format(y_example))\n",
    "print(\"Y_training example: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars))\n",
    "x = LSTM(activations)(model_input)\n",
    "x = Dense(n_chars, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "700/700 [==============================] - 1s 2ms/step - loss: 3.4296 - acc: 0.1114 - val_loss: 2.9438 - val_acc: 0.1033\n",
      "Epoch 2/50\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.9817 - acc: 0.1414 - val_loss: 2.8651 - val_acc: 0.2267\n",
      "Epoch 3/50\n",
      "700/700 [==============================] - 1s 903us/step - loss: 2.9401 - acc: 0.1714 - val_loss: 2.9128 - val_acc: 0.2267\n",
      "Epoch 4/50\n",
      "700/700 [==============================] - 1s 967us/step - loss: 2.9079 - acc: 0.1371 - val_loss: 2.8523 - val_acc: 0.2367\n",
      "Epoch 5/50\n",
      "700/700 [==============================] - 1s 979us/step - loss: 2.8377 - acc: 0.1929 - val_loss: 2.6932 - val_acc: 0.2267\n",
      "Epoch 6/50\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.7438 - acc: 0.1957 - val_loss: 2.7008 - val_acc: 0.2533\n",
      "Epoch 7/50\n",
      "700/700 [==============================] - 1s 912us/step - loss: 2.6200 - acc: 0.2414 - val_loss: 2.6145 - val_acc: 0.2800\n",
      "Epoch 8/50\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.5393 - acc: 0.2500 - val_loss: 2.5059 - val_acc: 0.3033\n",
      "Epoch 9/50\n",
      "700/700 [==============================] - 1s 900us/step - loss: 2.4438 - acc: 0.2657 - val_loss: 2.4440 - val_acc: 0.3367\n",
      "Epoch 10/50\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.3727 - acc: 0.2871 - val_loss: 2.4706 - val_acc: 0.2933\n",
      "Epoch 11/50\n",
      "700/700 [==============================] - 1s 937us/step - loss: 2.2290 - acc: 0.3414 - val_loss: 2.4383 - val_acc: 0.3100\n",
      "Epoch 12/50\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.1482 - acc: 0.3486 - val_loss: 2.4191 - val_acc: 0.3333\n",
      "Epoch 13/50\n",
      "700/700 [==============================] - 1s 921us/step - loss: 2.0818 - acc: 0.3500 - val_loss: 2.5087 - val_acc: 0.3133\n",
      "Epoch 14/50\n",
      "700/700 [==============================] - 1s 877us/step - loss: 1.9672 - acc: 0.3786 - val_loss: 2.4475 - val_acc: 0.3300\n",
      "Epoch 15/50\n",
      "700/700 [==============================] - 1s 924us/step - loss: 1.8533 - acc: 0.4257 - val_loss: 2.5674 - val_acc: 0.3033\n",
      "Epoch 16/50\n",
      "700/700 [==============================] - 1s 838us/step - loss: 1.7200 - acc: 0.4786 - val_loss: 2.5014 - val_acc: 0.3033\n",
      "Epoch 17/50\n",
      "700/700 [==============================] - 1s 814us/step - loss: 1.5912 - acc: 0.5200 - val_loss: 2.5092 - val_acc: 0.3333\n",
      "Epoch 18/50\n",
      "700/700 [==============================] - 1s 889us/step - loss: 1.4868 - acc: 0.5443 - val_loss: 2.5710 - val_acc: 0.2933\n",
      "Epoch 19/50\n",
      "700/700 [==============================] - 1s 767us/step - loss: 1.3392 - acc: 0.5871 - val_loss: 2.6042 - val_acc: 0.2967\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5227561160>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          Y_training, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = 'sweet dreams are made of thes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert new_sample to a sequence of one-hot encoded chars\n",
    "line_split = text_to_word_sequence(new_sample, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "char_split = list(\" \".join(line_split))\n",
    "n_sample_chars = len(char_split)\n",
    "\n",
    "sample = np.zeros((1, n_sample_chars, n_chars), dtype='float32')\n",
    "\n",
    "for ci, char in enumerate(char_split):\n",
    "    index = char_to_ix[char]\n",
    "    sample[0][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max of each...\n",
    "index = np.argmax(prediction[0])\n",
    "char = ix_to_char[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 29, 30)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sample\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of thes\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of thes se lint no t ne hode nown we\n",
      "t nt i wl woni te towe leriole aa cow  ne ttto\n",
      "b ne oonn lo iowt tose "
     ]
    }
   ],
   "source": [
    "sys.stdout.write(formatted_prediction)\n",
    "\n",
    "for i in range(100):\n",
    "    prediction = model.predict(x, verbose=0)[0]\n",
    "    # sample the predictions\n",
    "    next_index = sample_predictions(prediction)\n",
    "    next_char = ix_to_char[next_index]\n",
    "    string_prediction.append(next_char)\n",
    "    x[0][:-1] = x[0][1:]\n",
    "    x[0][-1] = prediction\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
