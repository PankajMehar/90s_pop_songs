{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Keras version 2.1.4 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n",
      "WARNING:root:TensorFlow version 1.8.0 detected. Last version known to be fully compatible is 1.5.0 .\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from coremltools import converters\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "activations = 128\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "training_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "output_dir = 'data/'\n",
    "charset_file = '{}charset.csv'.format(output_dir)\n",
    "dataset_file = '{}dataset.csv'.format(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocessPrediction(ix_to_char, prediction):\n",
    "    index = np.argmax(prediction)\n",
    "    char = ix_to_char[index]\n",
    "    \n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCharacterConverters(chars):\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "    \n",
    "    return char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n):\n",
    "    # create training input\n",
    "    x_dataset = np.zeros((n_examples, max_char_n-1, n_charset), dtype='float32')\n",
    "    \n",
    "    # create training input\n",
    "    y_dataset = np.zeros((n_examples, n_charset), dtype='float32')\n",
    "    \n",
    "    # fill input training set with word sequences, where words are one-hot encoded\n",
    "    for li, line in enumerate(dataset):\n",
    "        for ci, char in enumerate(line[:-1]):\n",
    "            index = char_to_ix[char]\n",
    "            x_dataset[li][ci][index] = 1\n",
    "            \n",
    "    # create training output\n",
    "    for li, line in enumerate(dataset):\n",
    "        char = line[-1]\n",
    "        index = char_to_ix[char]\n",
    "        y_dataset[li][index] = 1\n",
    "        \n",
    "    return x_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessExample(char_to_ix, example, n_chars_set):\n",
    "    chars = list(example)\n",
    "    n_sample_chars = len(chars)\n",
    "\n",
    "    preprocessed_example = np.zeros((1, n_sample_chars, n_chars_set), dtype='float32')\n",
    "\n",
    "    for ci, char in enumerate(chars):\n",
    "        index = char_to_ix[char]\n",
    "        preprocessed_example[0][ci][index] = 1\n",
    "\n",
    "    return preprocessed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(charset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    charset = []\n",
    "    for row in reader:\n",
    "        charset.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    dataset = []\n",
    "    for row in reader:\n",
    "        dataset.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 60\n",
      "ix_to_char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: ':', 17: ';', 18: '<', 19: '=', 20: '>', 21: '?', 22: '@', 23: '[', 24: '\\\\', 25: ']', 26: '^', 27: '_', 28: '`', 29: 'a', 30: 'b', 31: 'c', 32: 'd', 33: 'e', 34: 'f', 35: 'g', 36: 'h', 37: 'i', 38: 'j', 39: 'k', 40: 'l', 41: 'm', 42: 'n', 43: 'o', 44: 'p', 45: 'q', 46: 'r', 47: 's', 48: 't', 49: 'u', 50: 'v', 51: 'w', 52: 'x', 53: 'x', 54: 'y', 55: 'z', 56: '{', 57: '|', 58: '}', 59: '~'}\n",
      "char_to_ix: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, ':': 16, ';': 17, '<': 18, '=': 19, '>': 20, '?': 21, '@': 22, '[': 23, '\\\\': 24, ']': 25, '^': 26, '_': 27, '`': 28, 'a': 29, 'b': 30, 'c': 31, 'd': 32, 'e': 33, 'f': 34, 'g': 35, 'h': 36, 'i': 37, 'j': 38, 'k': 39, 'l': 40, 'm': 41, 'n': 42, 'o': 43, 'p': 44, 'q': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'x': 53, 'y': 54, 'z': 55, '{': 56, '|': 57, '}': 58, '~': 59}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix, ix_to_char = generateCharacterConverters(charset)\n",
    "n_charset = len(charset)\n",
    "\n",
    "print(\"Number of characters: {}\".format(n_charset))\n",
    "print(\"ix_to_char: {}\".format(ix_to_char))\n",
    "print(\"char_to_ix: {}\".format(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 100000\n",
      "max characters: 20\n"
     ]
    }
   ],
   "source": [
    "# create training input and output\n",
    "n_examples = len(dataset)\n",
    "max_char_n = len(dataset[0])\n",
    "x_dataset, y_dataset = generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n)\n",
    "print(\"Number of examples: {}\".format(n_examples))\n",
    "print(\"max characters: {}\".format(max_char_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_example shape: (19, 60)\n",
      "x_example one-hot: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "x_example: en you (when you)\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "x_example = x_dataset[2] \n",
    "\n",
    "x_example_string = []\n",
    "for woh in x_example:\n",
    "    char = deprocessPrediction(ix_to_char, woh)\n",
    "    x_example_string.append(char)\n",
    "x_example_string_formatted = \"\".join(x_example_string)\n",
    "\n",
    "print(\"x_example shape: {}\".format(x_example.shape))\n",
    "print(\"x_example one-hot: {}\".format(x_example))\n",
    "print(\"x_example: {}\".format(x_example_string_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_example shape: (60,)\n",
      "y_example one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_example: e\n"
     ]
    }
   ],
   "source": [
    "y_example = y_dataset[2]\n",
    "\n",
    "char = deprocessPrediction(ix_to_char, y_example)\n",
    "\n",
    "print(\"y_example shape: {}\".format(y_example.shape))\n",
    "print(\"y_example one-hot: {}\".format(y_example))\n",
    "print(\"y_example: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_charset))\n",
    "x = LSTM(activations)(model_input)\n",
    "x = Dense(n_charset, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate logging variables\n",
    "variant = '{}ex-{}a-{}b-{}c'.format(n_examples, activations, batch_size, max_char_n)\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "diagram_dir = 'diagram/{}-{}'.format(variant, timestamp)\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)\n",
    "mlmodel_dir = 'model/{}-{}.mlmodel'.format(variant, timestamp)\n",
    "model_dir = 'model/{}-{}.hdf5'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 133.00 191.00\" width=\"133pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 129,-187 129,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140566197527272 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140566197527272</title>\n",
       "<polygon fill=\"none\" points=\"-7.10543e-15,-146.5 -7.10543e-15,-182.5 125,-182.5 125,-146.5 -7.10543e-15,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140566197526712 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140566197526712</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-73.5 13.5,-109.5 111.5,-109.5 111.5,-73.5 13.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140566197527272&#45;&gt;140566197526712 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140566197527272-&gt;140566197526712</title>\n",
       "<path d=\"M62.5,-146.313C62.5,-138.289 62.5,-128.547 62.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-119.529 62.5,-109.529 59.0001,-119.529 66.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140566197530576 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140566197530576</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-0.5 11.5,-36.5 113.5,-36.5 113.5,-0.5 11.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140566197526712&#45;&gt;140566197530576 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140566197526712-&gt;140566197530576</title>\n",
       "<path d=\"M62.5,-73.3129C62.5,-65.2895 62.5,-55.5475 62.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-46.5288 62.5,-36.5288 59.0001,-46.5289 66.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a model diagram and save it to disk\n",
    "plot_model(model, to_file=diagram_dir)\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 60)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               96768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                7740      \n",
      "=================================================================\n",
      "Total params: 104,508\n",
      "Trainable params: 104,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70000 samples, validate on 30000 samples\n",
      "Epoch 1/10\n",
      "70000/70000 [==============================] - 57s 816us/step - loss: 2.1063 - acc: 0.3837 - val_loss: 1.8163 - val_acc: 0.4573\n",
      "Epoch 2/10\n",
      "70000/70000 [==============================] - 57s 807us/step - loss: 1.7312 - acc: 0.4810 - val_loss: 1.6907 - val_acc: 0.4918\n",
      "Epoch 3/10\n",
      "70000/70000 [==============================] - 57s 811us/step - loss: 1.6149 - acc: 0.5108 - val_loss: 1.6701 - val_acc: 0.5014\n",
      "Epoch 4/10\n",
      "70000/70000 [==============================] - 57s 811us/step - loss: 1.5421 - acc: 0.5309 - val_loss: 1.6350 - val_acc: 0.5121\n",
      "Epoch 5/10\n",
      "70000/70000 [==============================] - 57s 817us/step - loss: 1.4926 - acc: 0.5473 - val_loss: 1.6315 - val_acc: 0.5171\n",
      "Epoch 6/10\n",
      "70000/70000 [==============================] - 58s 822us/step - loss: 1.4569 - acc: 0.5544 - val_loss: 1.6227 - val_acc: 0.5166\n",
      "Epoch 7/10\n",
      "70000/70000 [==============================] - 57s 821us/step - loss: 1.4261 - acc: 0.5648 - val_loss: 1.6323 - val_acc: 0.5252\n",
      "Epoch 8/10\n",
      "70000/70000 [==============================] - 58s 826us/step - loss: 1.4020 - acc: 0.5713 - val_loss: 1.6358 - val_acc: 0.5194\n",
      "Epoch 9/10\n",
      "70000/70000 [==============================] - 58s 833us/step - loss: 1.3815 - acc: 0.5764 - val_loss: 1.6473 - val_acc: 0.5228\n",
      "Epoch 10/10\n",
      "70000/70000 [==============================] - 59s 839us/step - loss: 1.3636 - acc: 0.5816 - val_loss: 1.6598 - val_acc: 0.5239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7d98af438>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_dataset, \n",
    "          y_dataset, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'coremltools.converters.keras' has no attribute 'conver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0569f773d915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoreml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcoreml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'coremltools.converters.keras' has no attribute 'conver'"
     ]
    }
   ],
   "source": [
    "coreml_model = converters.keras.conver(model)\n",
    "coreml_model.save(mlmodel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
