{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from IPython.display import SVG\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "activations = 128\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "training_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "output_dir = 'data/'\n",
    "charset_file = '{}charset.csv'.format(output_dir)\n",
    "dataset_file = '{}dataset.csv'.format(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocessPrediction(ix_to_char, prediction):\n",
    "    index = np.argmax(prediction)\n",
    "    char = ix_to_char[index]\n",
    "    \n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCharacterConverters(chars):\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "    \n",
    "    return char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n):\n",
    "    # create training input\n",
    "    x_dataset = np.zeros((n_examples, max_char_n-1, n_charset), dtype='float32')\n",
    "    \n",
    "    # create training input\n",
    "    y_dataset = np.zeros((n_examples, n_charset), dtype='float32')\n",
    "    \n",
    "    # fill input training set with word sequences, where words are one-hot encoded\n",
    "    for li, line in enumerate(dataset):\n",
    "        for ci, char in enumerate(line[:-1]):\n",
    "            index = char_to_ix[char]\n",
    "            x_dataset[li][ci][index] = 1\n",
    "            \n",
    "    # create training output\n",
    "    for li, line in enumerate(dataset):\n",
    "        char = line[-1]\n",
    "        index = char_to_ix[char]\n",
    "        y_dataset[li][index] = 1\n",
    "        \n",
    "    return x_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessExample(char_to_ix, example, n_chars_set):\n",
    "    chars = list(example)\n",
    "    n_sample_chars = len(chars)\n",
    "\n",
    "    preprocessed_example = np.zeros((1, n_sample_chars, n_chars_set), dtype='float32')\n",
    "\n",
    "    for ci, char in enumerate(chars):\n",
    "        index = char_to_ix[char]\n",
    "        preprocessed_example[0][ci][index] = 1\n",
    "\n",
    "    return preprocessed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(charset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    charset = []\n",
    "    for row in reader:\n",
    "        charset.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=\",\")\n",
    "    dataset = []\n",
    "    for row in reader:\n",
    "        dataset.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 60\n",
      "ix_to_char: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: '+', 12: ',', 13: '-', 14: '.', 15: '/', 16: ':', 17: ';', 18: '<', 19: '=', 20: '>', 21: '?', 22: '@', 23: '[', 24: '\\\\', 25: ']', 26: '^', 27: '_', 28: '`', 29: 'a', 30: 'b', 31: 'c', 32: 'd', 33: 'e', 34: 'f', 35: 'g', 36: 'h', 37: 'i', 38: 'j', 39: 'k', 40: 'l', 41: 'm', 42: 'n', 43: 'o', 44: 'p', 45: 'q', 46: 'r', 47: 's', 48: 't', 49: 'u', 50: 'v', 51: 'w', 52: 'x', 53: 'x', 54: 'y', 55: 'z', 56: '{', 57: '|', 58: '}', 59: '~'}\n",
      "char_to_ix: {'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, ':': 16, ';': 17, '<': 18, '=': 19, '>': 20, '?': 21, '@': 22, '[': 23, '\\\\': 24, ']': 25, '^': 26, '_': 27, '`': 28, 'a': 29, 'b': 30, 'c': 31, 'd': 32, 'e': 33, 'f': 34, 'g': 35, 'h': 36, 'i': 37, 'j': 38, 'k': 39, 'l': 40, 'm': 41, 'n': 42, 'o': 43, 'p': 44, 'q': 45, 'r': 46, 's': 47, 't': 48, 'u': 49, 'v': 50, 'w': 51, 'x': 53, 'y': 54, 'z': 55, '{': 56, '|': 57, '}': 58, '~': 59}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix, ix_to_char = generateCharacterConverters(charset)\n",
    "n_charset = len(charset)\n",
    "\n",
    "print(\"Number of characters: {}\".format(n_charset))\n",
    "print(\"ix_to_char: {}\".format(ix_to_char))\n",
    "print(\"char_to_ix: {}\".format(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 300000\n",
      "max characters: 20\n"
     ]
    }
   ],
   "source": [
    "# create training input and output\n",
    "n_examples = len(dataset)\n",
    "max_char_n = len(dataset[0])\n",
    "x_dataset, y_dataset = generateXYDatasets(char_to_ix, dataset, n_examples, n_charset, max_char_n)\n",
    "print(\"Number of examples: {}\".format(n_examples))\n",
    "print(\"max characters: {}\".format(max_char_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_example shape: (19, 60)\n",
      "x_example one-hot: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "x_example: uenas\n",
      "con llevartel\n"
     ]
    }
   ],
   "source": [
    "x_example = x_dataset[2] \n",
    "\n",
    "x_example_string = []\n",
    "for woh in x_example:\n",
    "    char = deprocessPrediction(ix_to_char, woh)\n",
    "    x_example_string.append(char)\n",
    "x_example_string_formatted = \"\".join(x_example_string)\n",
    "\n",
    "print(\"x_example shape: {}\".format(x_example.shape))\n",
    "print(\"x_example one-hot: {}\".format(x_example))\n",
    "print(\"x_example: {}\".format(x_example_string_formatted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_example shape: (60,)\n",
      "y_example one-hot: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_example: a\n"
     ]
    }
   ],
   "source": [
    "y_example = y_dataset[2]\n",
    "\n",
    "char = deprocessPrediction(ix_to_char, y_example)\n",
    "\n",
    "print(\"y_example shape: {}\".format(y_example.shape))\n",
    "print(\"y_example one-hot: {}\".format(y_example))\n",
    "print(\"y_example: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_charset))\n",
    "x = LSTM(activations)(model_input)\n",
    "x = Dense(n_charset, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate logging variables\n",
    "variant = '{}ex-{}a-{}b-{}c'.format(n_examples, activations, batch_size, max_char_n)\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}-{}'.format(variant, timestamp)\n",
    "diagram_dir = 'diagram/{}-{}'.format(variant, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 133.00 191.00\" width=\"133pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 129,-187 129,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140274439053440 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140274439053440</title>\n",
       "<polygon fill=\"none\" points=\"-7.10543e-15,-146.5 -7.10543e-15,-182.5 125,-182.5 125,-146.5 -7.10543e-15,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140274439057192 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140274439057192</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-73.5 13.5,-109.5 111.5,-109.5 111.5,-73.5 13.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-87.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 140274439053440&#45;&gt;140274439057192 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140274439053440-&gt;140274439057192</title>\n",
       "<path d=\"M62.5,-146.313C62.5,-138.289 62.5,-128.547 62.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-119.529 62.5,-109.529 59.0001,-119.529 66.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140274439055064 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140274439055064</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-0.5 11.5,-36.5 113.5,-36.5 113.5,-0.5 11.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-14.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140274439057192&#45;&gt;140274439055064 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140274439057192-&gt;140274439055064</title>\n",
       "<path d=\"M62.5,-73.3129C62.5,-65.2895 62.5,-55.5475 62.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"66.0001,-46.5288 62.5,-36.5288 59.0001,-46.5289 66.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a model diagram and save it to disk\n",
    "plot_model(model, to_file=diagram_dir)\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, 60)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               96768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 60)                7740      \n",
      "=================================================================\n",
      "Total params: 104,508\n",
      "Trainable params: 104,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up callbacks\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210000 samples, validate on 90000 samples\n",
      "Epoch 1/10\n",
      "210000/210000 [==============================] - 178s 845us/step - loss: 1.8605 - acc: 0.4519 - val_loss: 1.6697 - val_acc: 0.5020\n",
      "Epoch 2/10\n",
      "210000/210000 [==============================] - 178s 848us/step - loss: 1.6057 - acc: 0.5192 - val_loss: 1.6087 - val_acc: 0.5196\n",
      "Epoch 3/10\n",
      "210000/210000 [==============================] - 180s 857us/step - loss: 1.5595 - acc: 0.5327 - val_loss: 1.5842 - val_acc: 0.5256\n",
      "Epoch 4/10\n",
      "210000/210000 [==============================] - 190s 903us/step - loss: 1.5761 - acc: 0.5393 - val_loss: 1.7038 - val_acc: 0.5272\n",
      "Epoch 5/10\n",
      "209900/210000 [============================>.] - ETA: 0s - loss: 1.6115 - acc: 0.5414"
     ]
    }
   ],
   "source": [
    "model.fit(x_dataset, \n",
    "          y_dataset, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          shuffle=True,\n",
    "          validation_split=training_ratio,\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'sweet dreams are made of these'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert example to a sequence of one-hot encoded chars\n",
    "preprocessed_example = preprocessExample(char_to_ix, example, n_charset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(preprocessed_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = deprocessPrediction(ix_to_char, prediction[0])\n",
    "\n",
    "print(\"Prediction: {}\".format(char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sys.stdout.write(example)\n",
    "\n",
    "for i in range(500):\n",
    "    prediction = model.predict(preprocessed_example, verbose=0)[0]\n",
    "    sampled_prediction = sample_predictions(prediction)\n",
    "    next_char = deprocessPrediction(ix_to_char, sampled_prediction[0])\n",
    "    preprocessed_example[0][:-1] = preprocessed_example[0][1:]\n",
    "    preprocessed_example[0][-1] = sampled_prediction\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
