{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "mask = (raw_data['year'] > 1989) & (raw_data['year'] < 2000) & (raw_data['genre'] == 'Pop') & (raw_data['lyrics'] != '[Instrumental]')\n",
    "filtered_data = raw_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any that have null values\n",
    "cleaned_data = filtered_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all the extra data. We only want the lyrics\n",
    "raw_lyrics = cleaned_data['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the lyrics to make it easier to work with\n",
    "reindexed_lyrics = raw_lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the lyrics to make it easier to work with\n",
    "formatted_lyrics = reindexed_lyrics[:].str.lower()\n",
    "formatted_lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "formatted_lyrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each lyric on \\n\n",
    "# store song lyrics as a list of lines\n",
    "# store those in lyrics\n",
    "lyrics_lines = []\n",
    "\n",
    "for i in range(len(formatted_lyrics)):\n",
    "    lyrics = formatted_lyrics[i].split('\\n')\n",
    "    lyrics_lines.append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flatten the previous into a list of song lyrics lines\n",
    "flattened_lyrics_lines = [line for song in lyrics_lines for line in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35188\n",
      "come they told me, pa rum pum pum pum\n"
     ]
    }
   ],
   "source": [
    "## examine the resulting number of song lyrics lines we have\n",
    "print(len(flattened_lyrics_lines))\n",
    "print(flattened_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "me llor y le cont\n"
     ]
    }
   ],
   "source": [
    "# grab a random amount of them for our examples\n",
    "from random import sample\n",
    "\n",
    "n_training = 700\n",
    "n_validation = 300\n",
    "\n",
    "examples = sample(flattened_lyrics_lines, n_training + n_validation)\n",
    "\n",
    "print(len(examples))\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# process lyrics into lists of word indices\n",
    "# also determine line with the greatest length\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import numpy as np\n",
    "\n",
    "max_char_n = 0\n",
    "chars = []\n",
    "\n",
    "for line in examples:\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    chars.append(char_split)\n",
    "    char_n = len(char_split)\n",
    "    if char_n > max_char_n:\n",
    "        max_char_n = char_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_char_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'x', 'r', 't', ' ', 'z', '2', 'u', 'f', '±', '¨', 'y', 'i', '1', \"'\", '8', 'q', 'b', '\\xad', '\\xa0', '6', '0', 'e', 'ª', 'ã', 'a', 'o', 'p', 'j', 'º', '¹', 'k', 'h', '¥', 'n', '³', '§', 'l', 'w', 'g', '9', '¶', '¤', '\\x80', 'c', '\\x87', 'v', 's', '¡', '©', 'm', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# flatten chars\n",
    "flat_chars = [item for sublist in chars for item in sublist]\n",
    "\n",
    "# dedup list\n",
    "chars = list(set(flat_chars))\n",
    "\n",
    "# append our terminator\n",
    "chars.append(\"\\n\")\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'd', 1: 'x', 2: 'r', 3: 't', 4: ' ', 5: 'z', 6: '2', 7: 'u', 8: 'f', 9: '±', 10: '¨', 11: 'y', 12: 'i', 13: '1', 14: \"'\", 15: '8', 16: 'q', 17: 'b', 18: '\\xad', 19: '\\xa0', 20: '6', 21: '0', 22: 'e', 23: 'ª', 24: 'ã', 25: 'a', 26: 'o', 27: 'p', 28: 'j', 29: 'º', 30: '¹', 31: 'k', 32: 'h', 33: '¥', 34: 'n', 35: '³', 36: '§', 37: 'l', 38: 'w', 39: 'g', 40: '9', 41: '¶', 42: '¤', 43: '\\x80', 44: 'c', 45: '\\x87', 46: 'v', 47: 's', 48: '¡', 49: '©', 50: 'm', 51: '\\n'}\n",
      "{'d': 0, 'x': 1, 'r': 2, 't': 3, ' ': 4, 'z': 5, '2': 6, 'u': 7, 'f': 8, '±': 9, '¨': 10, 'y': 11, 'i': 12, '1': 13, \"'\": 14, '8': 15, 'q': 16, 'b': 17, '\\xad': 18, '\\xa0': 19, '6': 20, '0': 21, 'e': 22, 'ª': 23, 'ã': 24, 'a': 25, 'o': 26, 'p': 27, 'j': 28, 'º': 29, '¹': 30, 'k': 31, 'h': 32, '¥': 33, 'n': 34, '³': 35, '§': 36, 'l': 37, 'w': 38, 'g': 39, '9': 40, '¶': 41, '¤': 42, '\\x80': 43, 'c': 44, '\\x87': 45, 'v': 46, 's': 47, '¡': 48, '©': 49, 'm': 50, '\\n': 51}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 94, 52)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training input\n",
    "X_training = np.zeros((n_training, max_char_n, n_chars), dtype='float32')\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill input training set with word sequences, where words are one-hot encoded\n",
    "for li, line in enumerate(examples[:n_training]):\n",
    "    indices = []\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(char_split):\n",
    "        index = char_to_ix[char]\n",
    "        X_training[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 95, 52)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training output\n",
    "Y_training = np.resize(X_training, (n_training, max_char_n + 1, n_chars))\n",
    "Y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs need to end with the end of sequence charecter\n",
    "for li, line in enumerate(X_training):\n",
    "    for ci, char in enumerate(line):\n",
    "        if :\n",
    "            Y_training[li][ci][-1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 94, 52)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation input\n",
    "X_validation = np.zeros((n_validation, max_char_n, n_chars), dtype='float32')\n",
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li, line in enumerate(examples[n_training:n_validation]):\n",
    "    indices = []\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(char_split):\n",
    "        index = char_to_ix[char]\n",
    "        X_validation[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 95, 52)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation output\n",
    "Y_validation = np.resize(X_validation, (n_validation, max_char_n + 1, n_chars))\n",
    "Y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs need to end with the end of sequence charecter\n",
    "for li, line in enumerate(X_validation):\n",
    "    Y_validation[li][-1][-1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me llor y le contddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd\n"
     ]
    }
   ],
   "source": [
    "x_training_string = []\n",
    "for woh in X_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    x_training_string.append(ix_to_char[max_idx])\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me llor y le contddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd\n"
     ]
    }
   ],
   "source": [
    "y_training_string = []\n",
    "for woh in Y_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    y_training_string.append(ix_to_char[max_idx])\n",
    "y_training_string_formatted = \"\".join(y_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars))\n",
    "x = LSTM(n_chars, return_sequences=True)(model_input)\n",
    "x = Dense(n_chars, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}'.format(timestamp)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "700/700 [==============================] - 2s 4ms/step - loss: 0.8664 - acc: 0.0912 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 2/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.3714 - acc: 0.2654 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 3/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.1054 - acc: 0.3031 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 4/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0392 - acc: 0.3045 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 5/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0170 - acc: 0.3052 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 6/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0079 - acc: 0.3054 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 7/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0040 - acc: 0.3055 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 8/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0023 - acc: 0.3056 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 9/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 0.0014 - acc: 0.3057 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 10/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 8.4058e-04 - acc: 0.3057 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 11/50\n",
      "700/700 [==============================] - 2s 3ms/step - loss: 5.7042e-04 - acc: 0.3057 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7997dd1908>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          X_training, \n",
    "          batch_size=50, \n",
    "          epochs=50, \n",
    "          validation_data=(X_validation, X_validation),\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = 'sweet dreams are made of these'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert new_sample to a sequence of one-hot encoded chars\n",
    "line_split = text_to_word_sequence(new_sample, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "char_split = list(\" \".join(line_split))\n",
    "n_sample_chars = len(char_split)\n",
    "\n",
    "sample = np.zeros((1, n_sample_chars, n_chars), dtype='float32')\n",
    "\n",
    "for ci, char in enumerate(char_split):\n",
    "    index = char_to_ix[char]\n",
    "    sample[0][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in prediction[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prediction = \"\".join(string_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    x = np.zeros((1, prediction.shape[1] + 1, n_chars), dtype='float32')\n",
    "    x[0][:prediction.shape[1]][:] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of these gee ie  eee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee fee  ed\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
