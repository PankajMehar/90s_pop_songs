{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating 90s Pop Lyrics at the Character level\n",
    "\n",
    "## Goal\n",
    "Generate 1 line of lyrics in the style of 90s Pop.\n",
    "\n",
    "## Problem Formulation\n",
    "X: Examples of a line of lyrics for the model to use (n_examples, max_length, n_characters)\n",
    "Y: A generated sequence of characters that ends with <EOS> (n_examples, n_characters)\n",
    "    \n",
    "<EOS> will be a special character in the vocabulary which the model will use to know that it can stop predicting.\n",
    "\n",
    "## Methodology\n",
    "To accomplish this, we need:\n",
    "1. Dataset: A corpus of 90s Pop lyrics\n",
    "2. Vocabulary: A set of characters which will be used for generating lyrics\n",
    "3. Model: A model which can encode the probability of the next character given a sequence of characters\n",
    "4. Generate Lyrics: Use the model and an input to generate new lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Transform Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data file as a dataframe\n",
    "raw_data = pd.read_csv('data/raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only lyrics from the 1990s, of the pop genre, and not instrumentals\n",
    "mask = (raw_data['year'] > 1989) & (raw_data['year'] < 2000) & (raw_data['genre'] == 'Pop') & (raw_data['lyrics'] != '[Instrumental]')\n",
    "filtered_data = raw_data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any that have null values\n",
    "cleaned_data = filtered_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim all the extra data. We only want the lyrics\n",
    "raw_lyrics = cleaned_data['lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the lyrics to make it easier to work with\n",
    "reindexed_lyrics = raw_lyrics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    come they told me, pa rum pum pum pum\\na new b...\n",
       "1    over the ground lies a mantle, white\\na heaven...\n",
       "2    i just came back from a lovely trip along the ...\n",
       "3    i'm dreaming of a white christmas\\njust like t...\n",
       "4    just hear those sleigh bells jingle-ing, ring-...\n",
       "5    little rump shaker she can really shake and ba...\n",
       "6    girl you want to sex me\\ngirl, why don't you l...\n",
       "7    oooh, tonight i want to turn the lights down l...\n",
       "8    so you say he let you on, you'll never give yo...\n",
       "9    something about you baby\\nthat makes me wanna ...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase the lyrics to make it easier to work with\n",
    "formatted_lyrics = reindexed_lyrics[:].str.lower()\n",
    "formatted_lyrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964\n"
     ]
    }
   ],
   "source": [
    "# examine the number of song lyrics we have\n",
    "n_formatted_lyrics = formatted_lyrics.shape[0]\n",
    "print(n_formatted_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each lyric on \\n\n",
    "# store song lyrics as a list of lines\n",
    "# store those in lyrics\n",
    "lyrics_lines = []\n",
    "\n",
    "for i in range(n_formatted_lyrics):\n",
    "    lyrics = formatted_lyrics[i].split('\\n')\n",
    "    lyrics_lines.append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the previous into a list of song lyrics lines\n",
    "flattened_lyrics_lines = [line for song in lyrics_lines for line in song]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35188\n",
      "come they told me, pa rum pum pum pum\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(flattened_lyrics_lines))\n",
    "print(flattened_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out non-english lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'x', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "english_lyrics_lines = []\n",
    "\n",
    "for line in flattened_lyrics_lines:\n",
    "    line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "    char_split = list(\" \".join(line_split))\n",
    "    char_check = 0\n",
    "    for char in char_split:\n",
    "        if char not in char_set:\n",
    "            char_check = 1\n",
    "            \n",
    "    if char_check == 0:\n",
    "        english_lyrics_lines.append(char_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33659\n",
      "['c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 'y', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ' ', 'p', 'a', ' ', 'r', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm', ' ', 'p', 'u', 'm']\n"
     ]
    }
   ],
   "source": [
    "# examine the resulting number of song lyrics lines we have\n",
    "print(len(english_lyrics_lines))\n",
    "print(english_lyrics_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the subset we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['f', 'a', 't', 'e', ' ', 's', 't', 'e', 'p', 's', ' ', 'i', 'n', ' ', 'a', 'n', 'd', ' ', 's', 'e', 'e', 's', ' ', 'y', 'o', 'u', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h']\n"
     ]
    }
   ],
   "source": [
    "# grab a random amount of them for our examples\n",
    "from random import sample\n",
    "\n",
    "n_training = 700\n",
    "n_validation = 300\n",
    "\n",
    "examples = sample(english_lyrics_lines, n_training + n_validation)\n",
    "\n",
    "print(len(examples))\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process lyrics into lists of word indices\n",
    "# also determine line with the greatest length\n",
    "max_char_n = 0\n",
    "\n",
    "for line in examples:\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    char_n = len(line)\n",
    "    if char_n > max_char_n:\n",
    "        max_char_n = char_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(max_char_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'f', 'l', 'x', 's', 'g', '2', 'c', 'm', 'q', 'v', '8', '9', 'w', 'h', 'y', 'd', 'n', 'a', 'z', ' ', 'p', '0', 'b', 'j', 'e', '1', 'r', 't', 'k', 'o', 'u', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# flatten chars\n",
    "flat_chars = [item for sublist in examples for item in sublist]\n",
    "\n",
    "# dedup list\n",
    "chars = list(set(flat_chars))\n",
    "\n",
    "# append our terminator\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# determine number of charecters in our set\n",
    "n_chars = len(chars)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: \"'\", 2: '0', 3: '1', 4: '2', 5: '8', 6: '9', 7: 'a', 8: 'b', 9: 'c', 10: 'd', 11: 'e', 12: 'f', 13: 'g', 14: 'h', 15: 'i', 16: 'j', 17: 'k', 18: 'l', 19: 'm', 20: 'n', 21: 'o', 22: 'p', 23: 'q', 24: 'r', 25: 's', 26: 't', 27: 'u', 28: 'v', 29: 'w', 30: 'x', 31: 'y', 32: 'z'}\n",
      "{' ': 0, \"'\": 1, '0': 2, '1': 3, '2': 4, '8': 5, '9': 6, 'a': 7, 'b': 8, 'c': 9, 'd': 10, 'e': 11, 'f': 12, 'g': 13, 'h': 14, 'i': 15, 'j': 16, 'k': 17, 'l': 18, 'm': 19, 'n': 20, 'o': 21, 'p': 22, 'q': 23, 'r': 24, 's': 25, 't': 26, 'u': 27, 'v': 28, 'w': 29, 'x': 30, 'y': 31, 'z': 32}\n"
     ]
    }
   ],
   "source": [
    "# create dictionarys\n",
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 70, 33)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create training input\n",
    "X_training = np.zeros((n_training, max_char_n, n_chars), dtype='float32')\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill input training set with word sequences, where words are one-hot encoded\n",
    "for li, line in enumerate(examples[:n_training]):\n",
    "    indices = []\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(line):\n",
    "        index = char_to_ix[char]\n",
    "        X_training[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 71, 33)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training output\n",
    "Y_training = np.resize(X_training, (n_training, max_char_n + 1, n_chars))\n",
    "Y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs need to end with the end of sequence charecter\n",
    "# for li, line in enumerate(X_training):\n",
    "#     spaceCounter = 0\n",
    "#     for ci, char in enumerate(line):\n",
    "#         if np.all(Y_training[li][ci] == 0):\n",
    "#             spaceCounter += 1\n",
    "#         if spaceCounter > 1:\n",
    "#             Y_training[li][ci-1][-1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 70, 33)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation input\n",
    "X_validation = np.zeros((n_validation, max_char_n, n_chars), dtype='float32')\n",
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li, line in enumerate(examples[n_training:n_validation]):\n",
    "    indices = []\n",
    "#     line_split = text_to_word_sequence(line, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "#     char_split = list(\" \".join(line_split))\n",
    "    for ci, char in enumerate(line):\n",
    "        index = char_to_ix[char]\n",
    "        X_validation[li][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 71, 33)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation output\n",
    "Y_validation = np.resize(X_validation, (n_validation, max_char_n + 1, n_chars))\n",
    "Y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # outputs need to end with the end of sequence charecter\n",
    "# for li, line in enumerate(X_validation):\n",
    "#     spaceCounter = 0\n",
    "#     for ci, char in enumerate(line):\n",
    "#         if np.all(Y_validation[li][ci] == 0):\n",
    "#             spaceCounter += 1\n",
    "#         if spaceCounter > 1:\n",
    "#             Y_validation[li][ci-1][-1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fate steps in and sees you through                                    \n"
     ]
    }
   ],
   "source": [
    "x_training_string = []\n",
    "for woh in X_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    x_training_string.append(ix_to_char[max_idx])\n",
    "x_training_string_formatted = \"\".join(x_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fate steps in and sees you through                                    \n"
     ]
    }
   ],
   "source": [
    "y_training_string = []\n",
    "for woh in Y_training[0]:\n",
    "    max_idx = np.argmax(woh)\n",
    "    y_training_string.append(ix_to_char[max_idx])\n",
    "y_training_string_formatted = \"\".join(y_training_string)\n",
    "print(x_training_string_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "# to use GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# verify that a gpu is listed\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, n_chars))\n",
    "x = LSTM(n_chars, return_sequences=True)(model_input)\n",
    "x = Dense(n_chars, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model_input, outputs=x)\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "log_dir = 'logs/{}'.format(timestamp)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc',\n",
    "                      min_delta=0,\n",
    "                      patience=10,\n",
    "                      verbose=1,\n",
    "                      mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (700, 70, 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-c5a6f5967da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           callbacks=[early, TensorBoard(log_dir=log_dir)])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1638\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1488\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1489\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have 2 dimensions, but got array with shape (700, 70, 33)"
     ]
    }
   ],
   "source": [
    "model.fit(X_training, \n",
    "          X_training, \n",
    "          batch_size=50, \n",
    "          epochs=50, \n",
    "          validation_data=(X_validation, X_validation),\n",
    "          callbacks=[early, TensorBoard(log_dir=log_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = 'sweet dreams are made of '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert new_sample to a sequence of one-hot encoded chars\n",
    "line_split = text_to_word_sequence(new_sample, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~')\n",
    "char_split = list(\" \".join(line_split))\n",
    "n_sample_chars = len(char_split)\n",
    "\n",
    "sample = np.zeros((1, n_sample_chars, n_chars), dtype='float32')\n",
    "\n",
    "for ci, char in enumerate(char_split):\n",
    "    index = char_to_ix[char]\n",
    "    sample[0][ci][index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in prediction[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prediction = \"\".join(string_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of\n"
     ]
    }
   ],
   "source": [
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a sequence from a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of\n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    x = np.zeros((1, prediction.shape[1] + 1, n_chars), dtype='float32')\n",
    "    x[0][:prediction.shape[1]][:] = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweet dreams are made of  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo  oo    \n"
     ]
    }
   ],
   "source": [
    "# take the max of each...\n",
    "string_prediction = []\n",
    "for p in x[0]:\n",
    "    max_p = np.argmax(p)\n",
    "    string_prediction.append(ix_to_char[max_p])\n",
    "formatted_prediction = \"\".join(string_prediction)\n",
    "print(formatted_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
